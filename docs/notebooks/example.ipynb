{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Quickstart `annbatch`"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will walk you through the following steps:\n",
    "1. How to convert an existing collection of `anndata` files into a shuffled, zarr-based, collection of `anndata` datasets\n",
    "2. How to load the converted collection using `annbatch`\n",
    "3. Extend an existing collection with new `anndata` datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download two example datasets from CELLxGENE\n",
    "!wget https://datasets.cellxgene.cziscience.com/866d7d5e-436b-4dbd-b7c1-7696487d452e.h5ad\n",
    "!wget https://datasets.cellxgene.cziscience.com/f81463b8-4986-4904-a0ea-20ff02cbb317.h5ad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTANT: Configure zarrs\n",
    "\n",
    "This step is both required for converting existing `anndata` files into a performant, shuffled collection of datasets for mini batch loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zarr\n",
    "import zarrs  # noqa\n",
    "\n",
    "zarr.config.set({\"codec_pipeline.path\": \"zarrs.ZarrsCodecPipeline\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Suppress zarr vlen-utf8 codec warnings\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=\"The codec `vlen-utf8` is currently not part in the Zarr format 3 specification.*\",\n",
    "    category=UserWarning,\n",
    "    module=\"zarr.codecs.vlen_utf8\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Converting existing `anndata` files into a shuffled collection"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The conversion code will take care of the following things:\n",
    "* Align (outer join) the gene spaces across all datasets listed in `adata_paths`\n",
    "  * The gene spaces are outer-joined based on the gene names provided in the `var_names` field of the individual `AnnData` objects.\n",
    "  * If you want to subset to specific gene space, you can provide a list of gene names via the `var_subset` parameter.\n",
    "* Shuffle the cells across all datasets (this works on larger than memory datasets as well).\n",
    "  * This is important for block-wise shuffling during data loading.\n",
    "* Shuffle the input files across multiple output datasets:\n",
    "  * The size of each individual output dataset can be controlled via the `n_obs_per_dataset` parameter.\n",
    "  * We recommend to choose a dataset size that comfortably fits into system memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arrayloaders import create_anndata_collection\n",
    "\n",
    "create_anndata_collection(\n",
    "    # List all the h5ad files you want to include in the collection\n",
    "    adata_paths=[\"866d7d5e-436b-4dbd-b7c1-7696487d452e.h5ad\", \"f81463b8-4986-4904-a0ea-20ff02cbb317.h5ad\"],\n",
    "    # Path to store the output collection\n",
    "    output_path=\"annbatch_collection\",\n",
    "    shuffle=True,  # Whether to pre-shuffle the cells of the collection\n",
    "    n_obs_per_dataset=2_097_152,  # Number of cells per dataset shard\n",
    "    var_subset=None,  # Optionally subset the collection to a specific gene space\n",
    "    should_denseify=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Data loading example"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "COLLECTION_PATH = Path(\"annbatch_collection/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anndata as ad\n",
    "\n",
    "from arrayloaders import ZarrSparseDataset\n",
    "\n",
    "ds = ZarrSparseDataset(\n",
    "    batch_size=4096,  # Total number of obs per yielded batch\n",
    "    chunk_size=256,  # Number of obs to load from disk contiguously - default settings should work well\n",
    "    preload_nchunks=32,  # Number of chunks to preload + shuffle - default settings should work well\n",
    "    preload_to_gpu=False,  # If True, preloaded chunks are moved to GPU memory via `cupy`, which can put more pressure on GPU memory but will accelerate loading ~20%\n",
    ")\n",
    "\n",
    "# Add dataset that should be used for training\n",
    "ds.add_anndatas(\n",
    "    [\n",
    "        ad.AnnData(\n",
    "            X=ad.io.sparse_dataset(zarr.open(p)[\"X\"]),\n",
    "            obs=ad.io.read_elem(zarr.open(p)[\"obs\"]),\n",
    "        )\n",
    "        for p in COLLECTION_PATH.glob(\"*.zarr\")\n",
    "    ],\n",
    "    obs_keys=\"cell_type\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT:**\n",
    "* The `ZarrSparseDataset` yields batches of sparse tensors.\n",
    "* The conversion to dense tensors should be done on the GPU, as shown in the example below.\n",
    "  * First call `.cuda()` and then `.to_dense()`\n",
    "  * E.g. `x = x.cuda().to_dense()`\n",
    "  * This is significantly faster than doing the dense conversion on the CPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over dataloader\n",
    "for batch in ds:\n",
    "    x, obs = batch\n",
    "    # Important: Convert to dense on GPU\n",
    "    x = x.cuda().to_dense()\n",
    "    # Feed data into your model\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Optional: Extend an existing collection with a new dataset"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might want to extend an existing pre-shuffled collection with a new dataset.\n",
    "This can be done using the `add_to_collection` function.\n",
    "\n",
    "This function will take care of shuffling the new dataset into the existing collection without having to re-shuffle the entire collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arrayloaders import add_to_collection\n",
    "\n",
    "add_to_collection(\n",
    "    adata_paths=[\n",
    "        \"866d7d5e-436b-4dbd-b7c1-7696487d452e.h5ad\",\n",
    "    ],\n",
    "    output_path=\"annbatch_collection\",\n",
    "    read_full_anndatas=True,  # This should be set to False if the new datasets DO NOT fit into memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('squidpy39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "ae6466e8d4f517858789b5c9e8f0ed238fb8964458a36305fca7bddc149e9c64"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
